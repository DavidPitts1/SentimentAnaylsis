{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "id": "xn7qyQPptgCx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 50000\n",
      "Normalizing Dataset\n",
      "Test Set\n",
      "Padding\n",
      "Max sentence length: 100\n",
      "Read embedding matrix\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "from tensorflow.python.keras import models, layers, optimizers\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import bz2\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, confusion_matrix\n",
    "import re\n",
    "from tensorflow.python.keras.layers import LSTM\n",
    "from tensorflow.python.keras.layers import RNN\n",
    "from tensorflow.python.keras.layers import GRU\n",
    "from tensorflow.keras.activations import tanh, sigmoid, relu, softmax\n",
    "import warnings\n",
    "from tensorflow.keras import models\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "def get_dataset():\n",
    "    movie_reviews = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "    train_texts = movie_reviews['review']\n",
    "    sent = movie_reviews['sentiment']\n",
    "    train_labels = list(map(lambda x: 1 if x == \"positive\" else 0, sent))\n",
    "\n",
    "    print(\"Dataset size: %d\" % len(train_labels))\n",
    "\n",
    "    p = np.random.permutation(len(train_texts))\n",
    "    train_texts = [train_texts[p[i]] for i in range(len(train_texts))]\n",
    "    train_labels = [train_labels[p[i]] for i in range(len(train_labels))]\n",
    "\n",
    "    # Normlaize\n",
    "\n",
    "    TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "    def remove_tags(text):\n",
    "        return TAG_RE.sub('', text)\n",
    "\n",
    "    def preprocess_text(sen):\n",
    "        # Removing html tags\n",
    "        sentence = remove_tags(sen)\n",
    "\n",
    "        # Remove punctuations and numbers\n",
    "        sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "        # Single character removal\n",
    "        sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "        # Removing multiple spaces\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "        return sentence\n",
    "\n",
    "    print(\"Normalizing Dataset\")\n",
    "\n",
    "    NON_ALPHANUM = re.compile(r'[\\W]')\n",
    "    NON_ASCII = re.compile(r'[^a-z0-1\\s]')\n",
    "\n",
    "    test_ascii = []\n",
    "\n",
    "    def normalize_texts(texts, labels):\n",
    "        normalized_texts = []\n",
    "        normalized_labels = []\n",
    "        for i in range(len(texts)):\n",
    "            text = texts[i]\n",
    "            label = labels[i]\n",
    "\n",
    "            lower = text.lower()\n",
    "\n",
    "            lower = preprocess_text(lower)\n",
    "\n",
    "            test_ascii.append(lower.split(\" \"))\n",
    "\n",
    "            normalized_texts.append(lower)\n",
    "            normalized_labels.append(label)\n",
    "            continue\n",
    "\n",
    "            no_punctuation = NON_ALPHANUM.sub(r' ', lower)\n",
    "            no_non_ascii = NON_ASCII.sub(r'', no_punctuation)\n",
    "            normalized_texts.append(no_non_ascii)\n",
    "        return normalized_texts, normalized_labels\n",
    "\n",
    "    train_texts, train_labels = normalize_texts(train_texts, train_labels)\n",
    "\n",
    "    train_labels = np.asarray(train_labels)\n",
    "\n",
    "    # validation\n",
    "    print(\"Test Set\")\n",
    "\n",
    "    MAX_FEATURES = 5000\n",
    "    # print(\"MAX_FEATURES: %d\" % MAX_FEATURES)\n",
    "    tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "    num_words = len(tokenizer.word_index.items())\n",
    "    MAX_FEATURES = num_words + 1\n",
    "\n",
    "    test_texts = train_texts[:500]\n",
    "    test_labels = train_labels[:500]\n",
    "    train_texts = train_texts[500:]\n",
    "    train_labels = train_labels[500:]\n",
    "    test_ascii = test_ascii[:500]\n",
    "\n",
    "    ##########################\n",
    "    # ADD YOUR OWN TEST TEXT #\n",
    "    ##########################\n",
    "\n",
    "    my_test_texts = []\n",
    "    my_test_texts.append(\"not extremely amazing good love like said\")\n",
    "    my_test_texts.append(\"very good movie i love movie very love \")\n",
    "    my_test_texts.append(\"you are so capable strong amazing and good looking that even writing your own name is easy for you\")\n",
    "    my_test_texts.append(\"bad move i hate it made me feel gross and bad and shallow really boring\")\n",
    "\n",
    "    ##########################\n",
    "    ##########################\n",
    "\n",
    "    for k in range(len(my_test_texts)):\n",
    "        test_texts[k] = my_test_texts[k]\n",
    "        test_ascii[k] = my_test_texts[k].split(\" \")\n",
    "\n",
    "    train_texts = tokenizer.texts_to_sequences(train_texts)\n",
    "    test_texts = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "    # pad\n",
    "    print(\"Padding\")\n",
    "    MAX_LENGTH = 100  # max(len(train_ex) for train_ex in train_texts)\n",
    "    print(\"Max sentence length: %d\" % MAX_LENGTH)\n",
    "    train_texts = pad_sequences(train_texts, padding='post', maxlen=MAX_LENGTH)\n",
    "    test_texts = pad_sequences(test_texts, padding='post', maxlen=MAX_LENGTH)\n",
    "\n",
    "    # read embedding mat\n",
    "    print(\"Read embedding matrix\")\n",
    "    embeddings_dictionary = dict()\n",
    "    glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "    word2ind=dict()\n",
    "    i=0\n",
    "    for line in glove_file:\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        word2ind[word]=i\n",
    "        i+=1\n",
    "        vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "        embeddings_dictionary[word] = vector_dimensions\n",
    "    glove_file.close()\n",
    "\n",
    "    embedding_matrix = np.zeros((num_words + 1, 100))\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_dictionary.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "\n",
    "    return train_texts, train_labels, test_texts, test_labels, test_ascii, embedding_matrix, MAX_LENGTH, MAX_FEATURES , embeddings_dictionary , word2ind , tokenizer\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "##########################\n",
    "# Code for Ex. #2 in IDL #\n",
    "##########################\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras import models, layers, optimizers\n",
    "import tensorflow as tf\n",
    "import bz2\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.setrecursionlimit(2500)\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "train_texts, train_labels, test_texts, test_labels, test_ascii, embedding_matrix, MAX_LENGTH, MAX_FEATURES , emb_dict,word2ind,token=get_dataset()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false,
    "id": "hiHieC1-tgC1",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1), ('and', 2), ('of', 3), ('to', 4), ('is', 5), ('it', 6), ('in', 7), ('this', 8), ('that', 9), ('was', 10), ('as', 11), ('movie', 12), ('for', 13), ('with', 14), ('but', 15), ('film', 16), ('you', 17), ('on', 18), ('not', 19), ('he', 20), ('are', 21), ('his', 22), ('have', 23), ('one', 24), ('be', 25)]\n",
      "['not', 'extremely', 'amazing', 'good', 'love', 'like', 'said']\n",
      "[ 19 554 483  45 106  34 300   0   0   0]\n",
      "ascii word : not , test word ind : 19 , tokenizer ind : 19\n",
      "ascii word : extremely , test word ind : 554 , tokenizer ind : 554\n",
      "ascii word : amazing , test word ind : 483 , tokenizer ind : 483\n",
      "ascii word : good , test word ind : 45 , tokenizer ind : 45\n",
      "ascii word : love , test word ind : 106 , tokenizer ind : 106\n",
      "ascii word : like , test word ind : 34 , tokenizer ind : 34\n",
      "ascii word : said , test word ind : 300 , tokenizer ind : 300\n"
     ]
    }
   ],
   "source": [
    "#Debugging the given code to find out the sub scores bugs\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "a=token.word_index\n",
    "b=list(token.word_index.items())[:25]\n",
    "print(b)\n",
    "\n",
    "print(test_ascii[0][:10])\n",
    "print(test_texts[0][:10])\n",
    "for ind,asci_word,test_text in zip(range(10),test_ascii[0][:10],test_texts[0][:10]):\n",
    "    print(\"ascii word : {} , test word ind : {} , tokenizer ind : {}\".format(asci_word,test_text,a[asci_word]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "id": "7oZTBLsNtgC1",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Code for Ex. #2 in IDL #\n",
    "##########################\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras import models, layers, optimizers\n",
    "import tensorflow as tf\n",
    "import bz2\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(2500)\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "#####################\n",
    "# Execusion options #\n",
    "#####################\n",
    "\n",
    "# Getting activations from model\n",
    "\n",
    "def get_act(net, input, name):\n",
    "\tsub_score = [layer for layer in net.layers if name in layer.name][0].output\n",
    "\t#functor = K.function([test_texts]+ [K.learning_phase()], sub_score)\n",
    "\tOutFunc = K.function([net.input], [sub_score])\n",
    "\treturn OutFunc([test_texts])[0]\n",
    "\t\n",
    "\t\n",
    "# RNN Cell Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "id": "tWZSTwpPtgC2",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def RNN(dim, x):\n",
    "    # Learnable weights in the cell\n",
    "    Wh = layers.Dense(dim, use_bias=False)\n",
    "    Wx = layers.Dense(dim)\n",
    "\n",
    "    # unstacking the time axis\n",
    "    x = tf.unstack(x, axis=1)\n",
    "\n",
    "    H = []\n",
    "\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        output_x=Wx(x[i])\n",
    "        if i==0:\n",
    "            h=Wh(Wx(x[i]))\n",
    "        else:\n",
    "            h=Wh(H[-1])\n",
    "            \n",
    "        H.append(relu(output_x+h))\n",
    "\n",
    "    H = tf.stack(H, axis=1)\n",
    "\n",
    "    return h, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false,
    "id": "1fqqQlIAtgC2",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GRU Cell Code\n",
    "\n",
    "def GRU(dim, x):\n",
    "    # Learnable weights in the cell\n",
    "    Wzx = layers.Dense(dim)\n",
    "    Uzh = layers.Dense(dim, use_bias=False)\n",
    "\n",
    "    Wrx = layers.Dense(dim)\n",
    "    Urh = layers.Dense(dim, use_bias=False)\n",
    "\n",
    "    Whx = layers.Dense(dim)\n",
    "    Uhh = layers.Dense(dim, use_bias=False)\n",
    "\n",
    "    # unstacking the time axis\n",
    "    x = tf.unstack(x, axis=1)\n",
    "\n",
    "    H = []\n",
    "\n",
    "    h = tf.zeros_like(Whx(x[0]))\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        if i != 0 :\n",
    "            h = H[-1]\n",
    "        wzx = Wzx(x[i])\n",
    "        wrx = Wrx(x[i])\n",
    "        whx = Whx(x[i])\n",
    "        z = sigmoid(wzx + Uzh(h))\n",
    "        r = sigmoid(wrx + Urh(h))\n",
    "        h_hat = tanh(whx + Uhh(h * r))\n",
    "        h = (1 - z) * h + z * h_hat\n",
    "        H.append(h)\n",
    "\n",
    "\n",
    "    H = tf.stack(H, axis=1)\n",
    "\n",
    "    return h, H\n",
    "\n",
    "# (Spatially-)Restricted Attention Layer\n",
    "# k - specifies the -k,+k neighbouring words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false,
    "id": "VHck9-TFtgC3",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def restricted_attention(x,k):\n",
    "    dim = x.shape[2]\n",
    "\n",
    "    Wq = layers.Dense(dim)\n",
    "    Wk = layers.Dense(dim)\n",
    "\n",
    "    wk = Wk(x)\n",
    "\n",
    "    paddings = tf.constant([[0, 0, ], [k, k], [0, 0]])\n",
    "    pk = tf.pad(wk, paddings)\n",
    "    pv = tf.pad(x, paddings)\n",
    "\n",
    "    keys = []\n",
    "    vals =   []\n",
    "    for i in range(-k, k + 1):\n",
    "        keys.append(tf.roll(pk, i, 1))\n",
    "        vals.append(tf.roll(pv, i, 1))\n",
    "\n",
    "    keys = tf.stack(keys, 2)\n",
    "    keys = keys[:, k:-k, :, :]\n",
    "    vals = tf.stack(vals, 2)\n",
    "    vals = vals[:, k:-k, :, :]\n",
    "\n",
    "    query = Wq(x)\n",
    "    dot_product = tf.einsum('lij,likj->lik', query, keys) / np.sqrt(dim)\n",
    "    attn_weights = layers.Softmax(name=\"atten_weights\", axis=1)(dot_product)\n",
    "    val_out = tf.einsum('lij,lijk->lik', attn_weights, vals)\n",
    "    return x + val_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false,
    "id": "W-blAcITtgC3",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "TRAIN = True\n",
    "\n",
    "RECR = True # recurrent netowrk (RNN/GRU) or a non-recurrent network\n",
    "\n",
    "ATTN = False # use attention layer in global sum pooling or not\n",
    "LSTM = True  # use LSTM or otherwise RNN\n",
    "\n",
    "\n",
    "def build_model(dim_FC_RNN=64, dim_FC_GRU=64):\n",
    "    sequences = layers.Input(shape=(MAX_LENGTH,))\n",
    "    embedding_layer = layers.Embedding(MAX_FEATURES, 100, weights=[embedding_matrix], input_length=MAX_LENGTH,\n",
    "                                       trainable=False)\n",
    "\n",
    "    # embedding the words into 100 dim vectors\n",
    "\n",
    "    x = embedding_layer(sequences)\n",
    "\n",
    "    if not RECR:\n",
    "\n",
    "        # non recurrent networks\n",
    "\n",
    "        if ATTN:\n",
    "            # attention layer\n",
    "            x = restricted_attention(x, k=5)\n",
    "\n",
    "        # word-wise FC layers -- MAKE SURE you have ,name= \"sub_score\" in the sub_scores step\n",
    "        # E.g., sub_score = layers.Dense(2,name=\"sub_score\")(x)\n",
    "\n",
    "        x = tf.unstack(x, axis=1)\n",
    "        net = tf.keras.Sequential()\n",
    "\n",
    "        for i in range(10):\n",
    "            net.add(layers.Dense(50, name=\"sub_score{}\".format(i), activation=\"relu\"))\n",
    "            net.add(layers.Dropout(0.1))\n",
    "\n",
    "        net.add(layers.Dense(32, activation=\"relu\"))\n",
    "        net.add(layers.Dense(2))\n",
    "\n",
    "        weight_pred=[]\n",
    "        for i in range(100):\n",
    "            word_pred = net(x[i])\n",
    "            weight_pred.append(word_pred)\n",
    "\n",
    "\n",
    "        weights_tf = (tf.convert_to_tensor(weight_pred, dtype=tf.float32))\n",
    "\n",
    "        preds=(weights_tf[:,:,0])\n",
    "        weights=(weights_tf[:,:,1])\n",
    "\n",
    "        weights_dist = softmax(weights,axis=0)\n",
    "        weighted_pred =(preds * weights_dist)\n",
    "\n",
    "        pre_pred = tf.math.reduce_sum(weighted_pred,axis=0)\n",
    "        sig =sigmoid(pre_pred)\n",
    "        predictions = tf.reshape(tf.convert_to_tensor(sig, dtype=tf.float32), shape=(-1, 1))\n",
    "\n",
    "    else:\n",
    "        # recurrent networks\n",
    "        if LSTM:\n",
    "            x, _ = GRU(dim_FC_GRU, x)\n",
    "        else:\n",
    "            x, _ = RNN(dim_FC_RNN, x)\n",
    "\n",
    "\n",
    "        x = layers.Dense(32, activation='relu')(x)\n",
    "        x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        predictions = x\n",
    "\n",
    "    model = models.Model(inputs=sequences, outputs=predictions)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['binary_accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "sTfbgViDtgC5",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "f1375857-2788-46cd-d362-e040baef43ec",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(RNN_DIM=64,GRU_DIM=64): \n",
    "    model = build_model(RNN_DIM,GRU_DIM)\n",
    "    if TRAIN:\n",
    "        print(\"Training\")\n",
    "        History=model.fit(\n",
    "            train_texts,\n",
    "            train_labels,\n",
    "            batch_size=128,\n",
    "            epochs=15,validation_data=(test_texts,test_labels))\n",
    "    return model , History\n",
    "\n",
    "model,History=train(GRU_DIM=64)\n",
    "preds = model.predict(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "sTfbgViDtgC5",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "f1375857-2788-46cd-d362-e040baef43ec",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Comapre diffenrent inner dimensions for the model\n",
    "# preds = model.predict(test_texts)\n",
    "# acc=[]\n",
    "# for i in np.arange(2,8)*10:\n",
    "#     model=train(GRU_DIM=i)\n",
    "#     preds = model.predict(test_texts)\n",
    "#     print('Accuracy score: for inner dim {} {:0.4}'.format(i,accuracy_score(test_labels, 1 * (preds > 0.5))))\n",
    "#     acc.append(accuracy_score(test_labels, 1 * (preds > 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "collapsed": false,
    "id": "OzhTrnjNtgC6",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "72993af6-8ab9-4943-8372-8354186eb7e9",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create a model that's cut at the 4th layer to get the predicitons for the subscores \\ weights , it was needed to change to 39 if we're dealing with attention layers.\n",
    "\n",
    "model2= models.Model(inputs=model.input, outputs=model.layers[4].output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "collapsed": false,
    "id": "OzhTrnjNtgC6",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "72993af6-8ab9-4943-8372-8354186eb7e9",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Scores for the word:  'bad'  with the following score -13.457328796386719\n",
      "Scores for the word:  'move'  with the following score 2.854335308074951\n",
      "Scores for the word:  'i'  with the following score 2.8893215656280518\n",
      "Scores for the word:  'hate'  with the following score 1.9233105182647705\n",
      "Scores for the word:  'it'  with the following score 2.416504144668579\n",
      "Scores for the word:  'made'  with the following score -6.323531150817871\n",
      "Scores for the word:  'me'  with the following score 4.552388668060303\n",
      "Scores for the word:  'feel'  with the following score 7.098709583282471\n",
      "Scores for the word:  'gross'  with the following score 2.6295955181121826\n",
      "Scores for the word:  'and'  with the following score 5.257828712463379\n",
      "Scores for the word:  'bad'  with the following score -13.457328796386719\n",
      "Scores for the word:  'and'  with the following score 5.257828712463379\n",
      "Scores for the word:  'shallow'  with the following score -16.907142639160156\n",
      "Scores for the word:  'really'  with the following score 2.0594303607940674\n",
      "Scores for the word:  'boring'  with the following score -21.266456604003906\n",
      "\n",
      "\n",
      "Negative\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>move</th>\n",
       "      <th>i</th>\n",
       "      <th>hate</th>\n",
       "      <th>it</th>\n",
       "      <th>made</th>\n",
       "      <th>me</th>\n",
       "      <th>feel</th>\n",
       "      <th>gross</th>\n",
       "      <th>and</th>\n",
       "      <th>shallow</th>\n",
       "      <th>really</th>\n",
       "      <th>boring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>-13.457329</td>\n",
       "      <td>2.854335</td>\n",
       "      <td>2.889322</td>\n",
       "      <td>1.923311</td>\n",
       "      <td>2.416504</td>\n",
       "      <td>-6.323531</td>\n",
       "      <td>4.552389</td>\n",
       "      <td>7.09871</td>\n",
       "      <td>2.629596</td>\n",
       "      <td>5.257829</td>\n",
       "      <td>-16.907143</td>\n",
       "      <td>2.05943</td>\n",
       "      <td>-21.266457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             bad      move         i      hate        it      made        me  \\\n",
       "score -13.457329  2.854335  2.889322  1.923311  2.416504 -6.323531  4.552389   \n",
       "\n",
       "          feel     gross       and    shallow   really     boring  \n",
       "score  7.09871  2.629596  5.257829 -16.907143  2.05943 -21.266457  "
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr=3\n",
    "sub_scores=model2.predict(test_texts[curr:curr+1])\n",
    "current_sentence=pd.DataFrame(columns=[\"score\"])\n",
    "for i in range(1):\n",
    "\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    if not RECR: \n",
    "        num = min((len(test_ascii[curr]), 100))\n",
    "        for k in range(num):\n",
    "            current_sentence.loc[test_ascii[curr][k]]=sub_scores[k,0,0]\n",
    "            print(\"Scores for the word:  '{}'  with the following score {}\".format(test_ascii[curr][k],sub_scores[k,0,0]))\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(test_ascii[i])\n",
    "        print(preds[i])\n",
    "\n",
    "    if preds[curr] > 0.5:\n",
    "        print(\"Positive\")\n",
    "    else:\n",
    "        print(\"Negative\")\n",
    "    print(\"-\" * 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "RB8VCs0ctgC6"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (7,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NbHuSIz8tgC7",
    "outputId": "b260fef8-ab42-49e0-da23-6f25ac4e1ff1"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'History'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-56a37886cc8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"binary_accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'History'"
     ]
    }
   ],
   "source": [
    "model.History[\"binary_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "103ip6AOtgC7",
    "outputId": "34a2c24e-5b1f-4d51-d4fd-c0a4142fef5a"
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.lineplot(np.arange(EPOCH_NUM),History.history[\"binary_accuracy\"],label=\"train acc\")\n",
    "sns.lineplot(np.arange(EPOCH_NUM),History.history[\"val_binary_accuracy\"],label=\"test acc\")\n",
    "plt.title(\"Q1.3 training process accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzjpzMCYtgC8",
    "outputId": "9fcbc1ca-5d0a-4cc3-bcdc-79da60f58e36",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ex2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
